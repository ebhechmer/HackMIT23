{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74f538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision #this package includes many of the most popular datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9ff235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "batch_size = 256\n",
    "data_path='./data/food_items'\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# for apple m1 or m2 chips\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# for devices other than apple, uncomment this line\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d021e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, 128]' is invalid for input of size 49152",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(trainloader))\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(class_names[labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[0;32m---> 25\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 128]' is invalid for input of size 49152"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load the entire dataset\n",
    "full_dataset = torchvision.datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "food_train, food_test = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "trainloader = DataLoader(food_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testloader = DataLoader(food_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "plt.title(class_names[labels[0].item()])\n",
    "plt.imshow(images[0].reshape(128, 128), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7fa8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 128, 128])\n",
      "OIL\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(class_names[labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a3371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=25, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 25)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3d4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2021607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of batches: {len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fbc3d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.063 | time: 1.11s\n",
      "[1,    11] loss: 0.629 | time: 10.89s\n",
      "[2,     1] loss: 0.062 | time: 15.73s\n",
      "[2,    11] loss: 0.618 | time: 25.40s\n",
      "[3,     1] loss: 0.058 | time: 30.24s\n",
      "[3,    11] loss: 0.582 | time: 39.92s\n",
      "[4,     1] loss: 0.056 | time: 44.78s\n",
      "[4,    11] loss: 0.559 | time: 54.45s\n",
      "[5,     1] loss: 0.056 | time: 59.31s\n",
      "[5,    11] loss: 0.556 | time: 68.97s\n",
      "[6,     1] loss: 0.057 | time: 73.82s\n",
      "[6,    11] loss: 0.550 | time: 83.52s\n",
      "[7,     1] loss: 0.054 | time: 88.35s\n",
      "[7,    11] loss: 0.544 | time: 98.03s\n",
      "[8,     1] loss: 0.055 | time: 102.86s\n",
      "[8,    11] loss: 0.546 | time: 112.57s\n",
      "[9,     1] loss: 0.055 | time: 117.44s\n",
      "[9,    11] loss: 0.549 | time: 127.16s\n",
      "[10,     1] loss: 0.053 | time: 131.98s\n",
      "[10,    11] loss: 0.543 | time: 141.70s\n",
      "[11,     1] loss: 0.054 | time: 146.55s\n",
      "[11,    11] loss: 0.548 | time: 156.25s\n",
      "[12,     1] loss: 0.054 | time: 161.11s\n",
      "[12,    11] loss: 0.540 | time: 170.80s\n",
      "[13,     1] loss: 0.056 | time: 175.67s\n",
      "[13,    11] loss: 0.547 | time: 185.43s\n",
      "[14,     1] loss: 0.055 | time: 190.27s\n",
      "[14,    11] loss: 0.563 | time: 199.98s\n",
      "[15,     1] loss: 0.056 | time: 204.85s\n",
      "[15,    11] loss: 0.547 | time: 214.52s\n",
      "[16,     1] loss: 0.055 | time: 219.38s\n",
      "[16,    11] loss: 0.542 | time: 229.10s\n",
      "[17,     1] loss: 0.054 | time: 233.94s\n",
      "[17,    11] loss: 0.543 | time: 243.67s\n",
      "[18,     1] loss: 0.057 | time: 248.49s\n",
      "[18,    11] loss: 0.552 | time: 258.21s\n",
      "[19,     1] loss: 0.053 | time: 263.09s\n",
      "[19,    11] loss: 0.549 | time: 272.82s\n",
      "[20,     1] loss: 0.054 | time: 277.68s\n",
      "[20,    11] loss: 0.549 | time: 287.41s\n",
      "[21,     1] loss: 0.054 | time: 292.27s\n",
      "[21,    11] loss: 0.537 | time: 302.00s\n",
      "[22,     1] loss: 0.053 | time: 306.85s\n",
      "[22,    11] loss: 0.532 | time: 316.56s\n",
      "[23,     1] loss: 0.053 | time: 321.42s\n",
      "[23,    11] loss: 0.535 | time: 331.20s\n",
      "[24,     1] loss: 0.053 | time: 336.13s\n",
      "[24,    11] loss: 0.532 | time: 345.90s\n",
      "[25,     1] loss: 0.053 | time: 350.76s\n",
      "[25,    11] loss: 0.535 | time: 360.47s\n",
      "[26,     1] loss: 0.053 | time: 365.30s\n",
      "[26,    11] loss: 0.537 | time: 375.03s\n",
      "[27,     1] loss: 0.053 | time: 379.91s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    248\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i%10 == 0):    \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 15:.3f} | time: {time.time() - start_time:.2f}s')\n",
    "            running_loss = 0.0\n",
    "end_time = time.time()  # Record end time\n",
    "print('Finished Training')\n",
    "print(f\"Training took {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60c613e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"BEANS\":2, \"CAKE\":5,'CANDY':3, \"CEREAL\":1,'CHIPS':1, 'CHOCOLATE':19,'COFFEE':17, 'CORN':0.7, 'FISH':13.6, 'FLOUR':1.4, 'HONEY':1.4, 'JAM':1.5, 'JUICE':1, 'MilK':5.4, 'NUTS':0.4, 'OIL':3.8, 'PASTA':1.6, 'RICE':3.6, 'SODA':1, 'SPICES':1,'SUGAR':0.4, 'TEA':31.5, 'TOMATO SAUCE':1.4, 'VINEGAR':1,'WATER':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "424a61bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "PATH = './model/model.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "# load model\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b5b963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 81.38%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # We don't need gradients for evaluation\n",
    "    for data in testloader:  # Assuming test_loader is your validation/test data loader\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Get the predicted class for each sample in the batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update total number of samples and number of correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3773424c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: CHOCOLATE\n",
      "Correct Class: CHOCOLATE\n",
      "Predicted Class: TEA\n",
      "Correct Class: TEA\n",
      "Predicted Class: PASTA\n",
      "Correct Class: PASTA\n",
      "Predicted Class: MILK\n",
      "Correct Class: MILK\n",
      "Predicted Class: FLOUR\n",
      "Correct Class: FLOUR\n",
      "Predicted Class: VINEGAR\n",
      "Correct Class: VINEGAR\n",
      "Predicted Class: SPICES\n",
      "Correct Class: SPICES\n",
      "Predicted Class: CANDY\n",
      "Correct Class: CANDY\n",
      "Predicted Class: TEA\n",
      "Correct Class: TEA\n",
      "Predicted Class: SODA\n",
      "Correct Class: SODA\n",
      "Predicted Class: SODA\n",
      "Correct Class: WATER\n",
      "Predicted Class: CANDY\n",
      "Correct Class: CANDY\n",
      "Predicted Class: COFFEE\n",
      "Correct Class: COFFEE\n",
      "Predicted Class: RICE\n",
      "Correct Class: RICE\n",
      "Predicted Class: SODA\n",
      "Correct Class: SODA\n",
      "Predicted Class: CHOCOLATE\n",
      "Correct Class: CHOCOLATE\n",
      "Predicted Class: CHOCOLATE\n",
      "Correct Class: CHOCOLATE\n",
      "Predicted Class: FISH\n",
      "Correct Class: FISH\n",
      "Predicted Class: SPICES\n",
      "Correct Class: SPICES\n",
      "Predicted Class: SPICES\n",
      "Correct Class: CORN\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(20):\n",
    "    images, labels = next(iter(testloader))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = net(images)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    print(f\"Predicted Class: {class_names[predicted_class[0]]}\")\n",
    "    print(f\"Correct Class: {class_names[labels[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0bc20e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that the image is: PASTA\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_image(image_path, model, transform, class_names):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure it's in RGB mode\n",
    "    \n",
    "    # Apply the same transformations as during training\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "    \n",
    "    # Move tensor to device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Usage\n",
    "image_path = \"images/pasta.png\"\n",
    "predicted_image = predict_image(image_path, net, transform, class_names)\n",
    "print(f\"The model predicts that the image is: {predicted_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a4998110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average carbon footprint of PASTA is 1.6kg.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average carbon footprint of {predicted_image} is {dic[predicted_image]}kg.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3121d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecofood",
   "language": "python",
   "name": "ecofood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
